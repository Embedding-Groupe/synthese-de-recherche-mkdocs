{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Bienvenue sur la documentation de Glosaurus","text":""},{"location":"#le-projet","title":"Le projet","text":"<p>Nous d\u00e9veloppons une application de bureau de type \"Linter de Glossaire Intelligent\". L'objectif est de faciliter l'\u00e9criture des termes m\u00e9tier dans le code, pour qu'il corresponde mieux au contexte m\u00e9tier.</p> <p>Notre outil vise \u00e0 r\u00e9soudre ce probl\u00e8me en offrant :</p> <ol> <li> <p>Un Gestionnaire de Glossaire centralis\u00e9.</p> </li> <li> <p>Une Assistance IA pour enrichir ce vocabulaire.</p> </li> <li> <p>Un V\u00e9rificateur de Code (Linter) pour garantir le respect des termes d\u00e9finis.</p> </li> </ol>"},{"location":"#fonctionnalites-principales","title":"Fonctionnalit\u00e9s principales","text":""},{"location":"#gestionnaire-de-glossaire","title":"Gestionnaire de Glossaire","text":"<p>Une interface compl\u00e8te pour cr\u00e9er, modifier, importer et exporter vos dictionnaires de donn\u00e9es.</p>"},{"location":"#intelligence-artificielle-locale-privee","title":"Intelligence artificielle locale (priv\u00e9e)","text":"<p>Nous avons int\u00e9gr\u00e9 un moteur d'IA l\u00e9ger mais performant permettant le fonctionnement enti\u00e8rement hors-ligne (pas de fuite de donn\u00e9es). Son r\u00f4le est de sugg\u00e9rer des synonymes, antonymes et contextes pour chaque mot ajout\u00e9.</p>"},{"location":"#analyseur-de-code-parser","title":"Analyseur de code (parser)","text":"<p>Le c\u0153ur de la v\u00e9rification.</p> <ul> <li> <p>Fonctionnement : L'utilisateur importe ses fichiers sources.</p> </li> <li> <p>Analyse : Le syst\u00e8me scanne le code (variables, fonctions, classes) et le compare au glossaire.</p> </li> <li> <p>Rapport : Il signale les termes inconnus ou propose des corrections bas\u00e9es sur le glossaire officiel.</p> </li> </ul>"},{"location":"#contenu-de-cette-documentation","title":"Contenu de cette documentation","text":"<p>Ce site a pour vocation de pr\u00e9senter notre d\u00e9marche de Recherche et D\u00e9veloppement. Vous y d\u00e9couvrirez :</p> <ul> <li>Les Recherches IA : Nos tests sur diff\u00e9rents mod\u00e8les (Word2Vec, BERT...), nos comparatifs de performances et la justification de nos choix finaux.</li> <li>L'Architecture du Parser : Comment nous transformons du code brut en donn\u00e9es analysables (AST, Tokenization).</li> <li>Les Choix Techniques : Front-end, Back-end, et int\u00e9gration des composants.</li> </ul> <p>Projet r\u00e9alis\u00e9 dans le cadre de la SAE de 3\u00e8me ann\u00e9e de BUT Informatique - IUT du Limousin.</p>"},{"location":"conclusion/","title":"Conclusion Personnelle","text":""},{"location":"conclusion/#ruben","title":"Ruben","text":"<p>Ce projet a \u00e9t\u00e9 tr\u00e8s enrichissant, tant sur le plan technique que sur la gestion de projet et la phase d'analyse. En effet, durant cette SAE, nous avons \u00e9chang\u00e9 \u00e0 de nombreuses reprises avec notre cliente pour faire \u00e9voluer le projet et r\u00e9pondre au mieux \u00e0 ses besoins, que ce soit pour le visuel (maquettes) ou pour les diff\u00e9rentes solutions de suggestion qui ont \u00e9volu\u00e9. Ces \u00e9changes ont men\u00e9 \u00e0 un pivot du projet qui, bien que d\u00e9routant (on a l'impression de perdre ce qui a \u00e9t\u00e9 fait), est int\u00e9ressant : il montre qu'il est possible de changer tant que cela reste pertinent et permet de mieux correspondre aux attentes du client.</p> <p>Du c\u00f4t\u00e9 technique, j'ai travaill\u00e9 majoritairement sur l'int\u00e9gration des suggestions par intelligence artificielle. Cela m'a permis d'apprendre \u00e9norm\u00e9ment sur l'utilisation d'IA, les embeddings et l'optimisation des prompts, ainsi que sur l'utilisation d'API et le lien entre le front et le back, particuli\u00e8rement lorsque l'on ajoute une couche comme Tauri.</p>"},{"location":"conclusion/#nathan","title":"Nathan","text":"<p>Durant ce projet, j'ai principalement travaill\u00e9 sur la partie back-end avec la mise en place du mod\u00e8le d'IA et sur le parser. Pour l'IA, nous avons d'abord d\u00fb monter en comp\u00e9tences car nous avions du mal \u00e0 comprendre le fonctionnement des mod\u00e8les. Apr\u00e8s plusieurs tests (utilisation d'outils externes, diff\u00e9rents mod\u00e8les), nous avons choisi un mod\u00e8le qui respectait nos contraintes : un mod\u00e8le l\u00e9ger (500 Mo) capable de r\u00e9pondre \u00e0 nos besoins (retour de synonymes, antonymes, etc.). La principale difficult\u00e9 a \u00e9t\u00e9 d'int\u00e9grer le mod\u00e8le dans l'application sans la surcharger.</p> <p>Pour ce qui est du parser, il n'y a pas eu de difficult\u00e9 majeure : il a surtout fallu effectuer un important travail de recherche en amont du d\u00e9veloppement pour obtenir les r\u00e9sultats souhait\u00e9s. La plus grosse partie du travail a \u00e9t\u00e9 r\u00e9alis\u00e9e (recherche de mots non-cl\u00e9s dans un script). Il reste \u00e0 ajouter le parcours de dossiers, le support de nouveaux langages et la liaison avec l'interface utilisateur. Nous pouvons \u00e9galement envisager l'ajout de tests unitaires pour garantir le bon fonctionnement de chaque composant.</p> <p>Pour conclure, je dirais que ce projet a \u00e9t\u00e9 et reste tr\u00e8s int\u00e9ressant. Bien que l'objectif principal \u00e9voqu\u00e9 en d\u00e9but d'ann\u00e9e ait \u00e9volu\u00e9 et que la partie IA soit moins importante que pr\u00e9vu, j'ai pu faire face \u00e0 de nouveaux probl\u00e8mes et d\u00e9couvrir ou red\u00e9couvrir des concepts tels que l'architecture d'une application, la mise en place d'un parser et la cr\u00e9ation d'un logiciel.</p>"},{"location":"conclusion/#maxence","title":"Maxence","text":"<p>Ce projet m\u2019a particuli\u00e8rement plu par son caract\u00e8re \u00e9volutif : le pivot effectuer ainsi que les retours r\u00e9guliers de la cliente m'ont appris \u00e0 mieux recueillir et formaliser les besoins. Sur la partie d\u00e9veloppement, je me suis occup\u00e9 du maquettage et du front\u2011end de l\u2019application (prototypage sous Figma, int\u00e9gration des maquettes), ce qui m\u2019a permis d\u2019am\u00e9liorer mes comp\u00e9tences en design d\u2019interface et en int\u00e9gration. J\u2019ai \u00e9galement d\u00e9couvert de nouvelles technologies comme Electron et des outils d\u2019IA g\u00e9n\u00e9rative centralis\u00e9e (Stitch), qui ont enrichi mon approche du prototypage.</p> <p>Sur le plan technique, j\u2019ai d\u00e9couvert les bases des embeddings et du fonctionnement des mod\u00e8les de langage. Je n\u2019ai pas particip\u00e9 au d\u00e9veloppement back\u2011end, mais j\u2019ai travaill\u00e9 en collaboration avec ceux en charge de celui-ci et assist\u00e9 aux choix architecturaux de l'application (souverainet\u00e9 des donn\u00e9es, etc).</p>"},{"location":"conclusion/#alexis","title":"Alexis","text":"<p>Ce projet m\u2019a permis d\u2019acqu\u00e9rir des connaissances et comp\u00e9tences sur les embeddings, une notion de l\u2019informatique de plus en plus pr\u00e9sente aujourd\u2019hui. Avant ce projet, je n\u2019avais qu\u2019une compr\u00e9hension tr\u00e8s g\u00e9n\u00e9rale de ce concept. Gr\u00e2ce aux recherches et exp\u00e9rimentations que nous avons r\u00e9alis\u00e9es, j\u2019ai pu comprendre comment les embeddings fonctionnent, comment ils sont utilis\u00e9s pour repr\u00e9senter des donn\u00e9es sous forme de vecteurs et comment ils peuvent am\u00e9liorer des applications comme le traitement de texte ou la recherche intelligente.</p> <p>Personnellement, je me suis davantage concentr\u00e9 sur le front-end de l\u2019application, car c\u2019est l\u2019une des parties du d\u00e9veloppement que je pr\u00e9f\u00e8re. Au cours de cette phase, j\u2019ai acquis des comp\u00e9tences dans diff\u00e9rents domaines :</p> <ul> <li> <p>Maquettage et design : j\u2019ai utilis\u00e9 Figma pour la premi\u00e8re fois afin de cr\u00e9er des maquettes et prototyper l\u2019interface. J\u2019ai d\u00e9couvert que cet outil est tr\u00e8s utile, mais \u00e9galement assez complexe \u00e0 prendre en main.</p> </li> <li> <p>Qualit\u00e9 de d\u00e9veloppement : j\u2019ai appris \u00e0 mettre en place des tests unitaires pour le front-end.</p> </li> <li> <p>Gestion de projet : nous avons organis\u00e9 des sessions de brainstorming et planifi\u00e9 nos t\u00e2ches \u00e0 l\u2019aide d\u2019outils comme Miro, ce qui m\u2019a permis de me familiariser avec la coordination d\u2019\u00e9quipe et la planification agile.</p> </li> </ul> <p>En ce qui concerne les difficult\u00e9s rencontr\u00e9es, je n\u2019ai pas connu de point de blocage particulier. Cependant, une erreur que j\u2019ai commise a \u00e9t\u00e9 de ne pas avoir correctement organis\u00e9 l\u2019architecture du projet d\u00e8s le d\u00e9part. Cela nous a fait perdre du temps, car nous avons d\u00fb r\u00e9organiser l\u2019ensemble des fichiers par la suite afin d\u2019obtenir une structure plus propre et plus claire. C\u2019est un point sur lequel je dois m\u2019am\u00e9liorer.</p> <p>Enfin, ce projet m\u2019a donn\u00e9 l\u2019opportunit\u00e9 de travailler sur l\u2019int\u00e9gration d\u2019outils modernes comme Tauri pour le front-end et FastAPI pour le back-end. Cela m\u2019a permis de mieux comprendre l\u2019architecture globale d\u2019une application ainsi que la communication entre le front-end et le back-end. Ce projet a renforc\u00e9 mes comp\u00e9tences techniques tout en me sensibilisant \u00e0 l\u2019importance de la collaboration et de l\u2019organisation dans un projet de d\u00e9veloppement logiciel en \u00e9quipe. Pour moi, il a \u00e9t\u00e9 particuli\u00e8rement agr\u00e9able et gratifiant de voir notre projet \u00e9voluer au fil du temps, et je suis fier du travail que nous avons r\u00e9alis\u00e9 ensemble.</p>"},{"location":"contexte_besoin/","title":"Contexte et besoins du projet","text":""},{"location":"contexte_besoin/#utilisation-des-embeddings","title":"Utilisation des embeddings","text":"<p>L\u2019intelligence artificielle, en particulier les LLM, est omnipr\u00e9sente parmi nous de nos jours. Cependant, pour que ceux-ci puissent r\u00e9pondre \u00e0 nos demandes, ils doivent \u00eatre capables de comprendre nos mots. Malheureusement, l\u2019intelligence artificielle ne comprend que les nombres et les math\u00e9matiques. C\u2019est pourquoi nos mots sont transform\u00e9s en tokens \u2014 des mots ou des fractions de mots \u2014 qui sont ensuite convertis en vecteurs : ce sont les embeddings. Ces vecteurs sont tr\u00e8s complexes et peuvent contenir des milliers de valeurs chacun.</p> <p>En manipulant les embeddings, puis en les simplifiant en trois dimensions pour les observer dans l\u2019espace, on peut constater de nombreuses choses int\u00e9ressantes :</p> <ul> <li>Contexte : des mots utilis\u00e9s dans le m\u00eame contexte ou appartenant au m\u00eame champ lexical se positionnent plut\u00f4t proches dans l\u2019espace.  </li> <li>Op\u00e9rations : on peut effectuer des op\u00e9rations sur les vecteurs des mots et, en r\u00e9cup\u00e9rant le vecteur du mot le plus proche du r\u00e9sultat, obtenir un mot coh\u00e9rent.   Par exemple : <code>roi - homme + femme = reine</code>.   En d\u00e9taillant, <code>roi - homme</code> fait ressortir le principe de la royaut\u00e9, et si l\u2019on ajoute <code>femme</code> \u00e0 ce principe, on obtient naturellement <code>reine</code> :</li> </ul> <p></p> <p>Mais ces op\u00e9rations ne se limitent pas \u00e0 cela : on peut \u00e9galement appliquer des principes \u00e0 des mots. Par exemple, si l\u2019on ajoute le vecteur <code>synonyme</code> au vecteur <code>vite</code>, on obtiendra probablement un vecteur proche de celui de <code>rapide</code>.</p>"},{"location":"contexte_besoin/#application-concrete","title":"Application concr\u00e8te","text":"<p>De ces constats est n\u00e9e l\u2019id\u00e9e d\u2019appliquer ce principe \u00e0 des cas plus concrets. C\u2019est pourquoi nous avons travaill\u00e9 sur la suggestion intelligente de termes m\u00e9tier, notamment pour le nommage de variables dans le cadre de la programmation, afin d\u2019am\u00e9liorer la lisibilit\u00e9, la maintenabilit\u00e9 et l\u2019auto-documentation du code. Dans un premier temps, il a donc \u00e9t\u00e9 d\u00e9cid\u00e9 de mieux comprendre la manipulation des embeddings, pour ensuite int\u00e9grer la suggestion intelligente dans diff\u00e9rentes applications : un glossaire m\u00e9tier et une extension d\u2019environnement de d\u00e9veloppement.</p>"},{"location":"contexte_besoin/#projet-dapplication-glossaire","title":"Projet d\u2019application \u00ab glossaire \u00bb","text":"<p>Le but du projet est de d\u00e9velopper une application de glossaire m\u00e9tier qui permettra de saisir un mot et ses informations : d\u00e9finition, contexte, synonymes, antonymes, termes li\u00e9s, etc. \u00c0 cette application, il faudra ajouter des suggestions intelligentes, en utilisant l\u2019intelligence artificielle et les embeddings, afin de compl\u00e9ter les informations d\u2019un mot de mani\u00e8re coh\u00e9rente et pertinente.</p>"},{"location":"contexte_besoin/#cibles-et-contraintes","title":"Cibles et contraintes","text":"<p>Cette application est destin\u00e9e \u00e0 diff\u00e9rents types d'utilisateurs, qui apportent chacun leur lot de contraintes :</p> <ul> <li>Les \u00e9tudiants : Notre cliente \u00e9tant enseignante \u00e0 l'IUT, elle aimerait pouvoir utiliser l'application pour ses \u00e9l\u00e8ves. Pour les \u00e9tudiants, il y a deux cas : les ordinateurs de l'IUT ou les ordinateurs personnels. Pour les ordinateurs de l'IUT, l'application sera disponible depuis des machines virtuelles : il faut donc qu'elle soit l\u00e9g\u00e8re et que le mod\u00e8le d'IA puisse tourner en local sur ces machines, sous Linux.</li> </ul> <p>Pour les \u00e9tudiants qui utilisent une machine personnelle, il faut r\u00e9pondre \u00e0 la m\u00eame contrainte de performance, car certains poss\u00e8dent de vieilles machines peu performantes. De plus, l'application doit pouvoir tourner sur tous les syst\u00e8mes d'exploitation (Linux, Windows, macOS).</p> <ul> <li> <p>Les entreprises : Notre cliente poss\u00e8de \u00e9galement un r\u00e9seau de professionnels int\u00e9ress\u00e9s par cette application et qui aimeraient l'utiliser pour leurs projets. Cela ne pose pas de contrainte de performance particuli\u00e8re ; cependant, les donn\u00e9es ins\u00e9r\u00e9es dans l'application peuvent \u00eatre sensibles (glossaire, code), surtout si l'on utilise de l'IA. Cela impose de respecter la souverainet\u00e9 des donn\u00e9es et d'assurer qu'elles ne sont pas publi\u00e9es dans des mod\u00e8les d'IA ou tout autre service.</p> </li> <li> <p>Open-source : l'application est \u00e9galement destin\u00e9e \u00e0 \u00eatre open-source, pour permettre son \u00e9volution et recevoir des suggestions de la communaut\u00e9, et pour qu'elle puisse \u00eatre reprise et r\u00e9utilis\u00e9e pour d'autres projets (SAE ou recherche). Cela impose de v\u00e9rifier que notre utilisation d'outils externes, comme des mod\u00e8les d'IA, n'entre pas en conflit avec leurs licences et conditions d'utilisation.</p> </li> </ul>"},{"location":"front-end/","title":"Langage et framework front\u2011end pour l\u2019application FastAPI","text":"<p>Pour d\u00e9velopper le front-end, c\u2019est-\u00e0-dire l\u2019aspect visuel de l\u2019application, il est n\u00e9cessaire de r\u00e9fl\u00e9chir aux langages, frameworks \u00e0 utiliser. C\u2019est pourquoi nous avons men\u00e9 une r\u00e9flexion approfondie sur le choix des technologies, afin de d\u00e9terminer celles qui offrent les meilleures performances pour l\u2019application tout en r\u00e9pondant aux besoins de l\u2019utilisateur.</p>"},{"location":"front-end/#besoin-du-client","title":"Besoin du client","text":"<p>En ce qui concerne le front-end, le client souhaite une application fonctionnelle, mais \u00e9galement esth\u00e9tique, agr\u00e9able visuellement et moderne, tout en respectant la charte graphique de l\u2019IUT.</p>"},{"location":"front-end/#presentation-des-frameworks-front-end","title":"Pr\u00e9sentation des frameworks front-end","text":"<p>Pour accompagner FastAPI et r\u00e9pondre aux besoins du client, plusieurs frameworks front-end modernes peuvent \u00eatre envisag\u00e9s.</p> <ul> <li> <p>React est l\u2019un des frameworks les plus populaires. Il repose sur JavaScript ou TypeScript et offre un \u00e9cosyst\u00e8me tr\u00e8s complet. Sa structure bas\u00e9e sur des composants r\u00e9utilisables permet de concevoir des interfaces dynamiques et modulaires. Gr\u00e2ce \u00e0 des biblioth\u00e8ques comme Axios, React s\u2019int\u00e8gre parfaitement avec FastAPI pour des requ\u00eates HTTP. Ce framework est id\u00e9al pour les applications modernes. Cependant, React est relativement lourd \u00e0 mettre en place, surtout pour des petites applications, car il n\u00e9cessite le chargement de nombreux fichiers JavaScript avant que l\u2019application ne soit utilisable. Le navigateur doit t\u00e9l\u00e9charger, analyser et ex\u00e9cuter ce code, ce qui allonge le temps de chargement initial. De plus, l\u2019utilisation de multiples biblioth\u00e8ques et outils de configuration augmente la complexit\u00e9 technique du projet. Dans certains cas, des applications peuvent atteindre une taille de 2,3 Mo, ce qui t\u00e9moigne d\u2019une faible optimisation.</p> </li> <li> <p>Vue.js est un framework front-end bas\u00e9 sur JavaScript. Parmi ses principaux avantages, on trouve sa courbe d\u2019apprentissage tr\u00e8s accessible, sa documentation compl\u00e8te, et sa l\u00e9g\u00e8ret\u00e9, qui le rendent id\u00e9al pour des projets de petite \u00e0 moyenne envergure. Il offre \u00e9galement une bonne int\u00e9gration avec des backends comme FastAPI avec sa gestion simple des appels API. Cependant, Vue pr\u00e9sente aussi quelques inconv\u00e9nients. En effet, son \u00e9cosyst\u00e8me est plus restreint que celui de React et peut parfois limiter le choix de biblioth\u00e8ques et d\u2019outils.</p> </li> <li> <p>Preact est une version all\u00e9g\u00e9e de React, offrant la m\u00eame syntaxe et les m\u00eames concepts mais avec un poids bien inf\u00e9rieur et des performances accrues. En effet, Preact est plus rapide parce qu\u2019il utilise un Virtual DOM plus simple que React. Il fait moins de calculs et de v\u00e9rifications avant de modifier le DOM r\u00e9el, ce qui permet d\u2019appliquer les changements plus rapidement et avec moins de code \u00e0 ex\u00e9cuter. Il est compatible avec la plupart des biblioth\u00e8ques React via preact/compat, il est id\u00e9al pour des applications l\u00e9g\u00e8res et rapides. Une application avec <code>preact/compat</code> a une taille d'environ 9 ko. En r\u00e9sum\u00e9, Preact est l'\u00e9quivalent de React sans ses d\u00e9fauts.</p> </li> </ul>"},{"location":"front-end/#conclusion","title":"Conclusion","text":"<p>Tous les frameworks front-end pr\u00e9sent\u00e9s utilisent le langage JavaScript. On peut donc en d\u00e9duire que ce langage est adapt\u00e9 pour r\u00e9pondre aux besoins exprim\u00e9s, notamment en mati\u00e8re d\u2019interactivit\u00e9, de dynamisme et d\u2019int\u00e9gration avec une API FastAPI. Parmi ces frameworks, Preact appara\u00eet comme le meilleur choix pour un projet de petite envergure, gr\u00e2ce \u00e0 sa l\u00e9g\u00e8ret\u00e9, ses excellentes performances et sa compatibilit\u00e9 avec l\u2019\u00e9cosyst\u00e8me React. Il offre ainsi la possibilit\u00e9 de b\u00e9n\u00e9ficier des avantages de React en r\u00e9duisant les temps de chargement. Son \u00e9cosyst\u00e8me complet, sa facilit\u00e9 d\u2019int\u00e9gration et sa simplicit\u00e9 de mise en \u0153uvre en font une solution particuli\u00e8rement adapt\u00e9e pour d\u00e9velopper une interface moderne, fluide et performante en lien avec un backend FastAPI et qui r\u00e9pond parfaitement aux besoins du client.</p>"},{"location":"maquettage/","title":"L'\u00e9volution des maquettes","text":"<p>Pour ce projet, nous avons r\u00e9alis\u00e9 des maquettes et des prototypes destin\u00e9s \u00e0 \u00eatre pr\u00e9sent\u00e9s au client afin qu\u2019il valide le design de l\u2019application avant de commencer le d\u00e9veloppement.</p> <p>Au d\u00e9part, nous avions quelques difficult\u00e9s avec le design. C\u2019est pourquoi nous avons utilis\u00e9 une intelligence artificielle g\u00e9n\u00e9rative de maquettes que nous avons d\u00e9couverte : Stitch. Apr\u00e8s plusieurs prompts d\u00e9taill\u00e9s, l\u2019IA nous a g\u00e9n\u00e9r\u00e9 des maquettes plut\u00f4t convaincantes, qui nous ont servi de base pour concevoir le design final de l\u2019application \u00e0 l\u2019aide de l\u2019outil Figma.</p> <p></p> <p>Une fois cette base obtenue, il nous a fallu cr\u00e9er un logo. Pour cela, nous avons \u00e9galement utilis\u00e9 une IA g\u00e9n\u00e9rative, en nous basant sur le nom de l\u2019application (Glosaurus) ainsi que sur son fonctionnement.</p> <p></p> <p>Tout au long du processus de maquettage, nous avons re\u00e7u des retours r\u00e9guliers du client. Ceux-ci nous ont permis d\u2019affiner le design en fonction de ses remarques et des besoins exprim\u00e9s, notamment en int\u00e9grant la charte graphique de l\u2019Universit\u00e9 de Limoges. Finalement, l\u2019ensemble de ces \u00e9changes avec le client nous a conduit \u00e0 un r\u00e9sultat final moderne et \u00e9pur\u00e9, tout en respectant pleinement ses attentes et ses besoins.</p> <p></p>"},{"location":"parser/","title":"R\u00e9cup\u00e9ration des mots\u2011cl\u00e9s (Parser)","text":""},{"location":"parser/#contexte-dutilisation","title":"Contexte d'utilisation","text":"<p>Dans le cadre de notre application, nous souhaitons analyser des fichiers de code source afin d'en extraire les concepts cl\u00e9s (les noms de variables, fonctions, classes, etc.). L'objectif est de transformer un code brut en une liste structur\u00e9e de mots significatifs pond\u00e9r\u00e9s par leur fr\u00e9quence. Ces donn\u00e9es sont destin\u00e9es \u00e0 alimenter une intelligence artificielle ou un moteur de recherche interne, permettant de classifier ou de r\u00e9sumer le contenu technique d'un projet. De plus, elles nous serviront plus tard pour pouvoir v\u00e9rifier si tous les mots d'un glossaire sont pr\u00e9sents dans le code, \u00e0 la mani\u00e8re d'un linter.</p>"},{"location":"parser/#solutions-envisagees","title":"Solutions envisag\u00e9es","text":"<p>Pour parvenir \u00e0 extraire ces informations, plusieurs approches ont \u00e9t\u00e9 consid\u00e9r\u00e9es :</p> <ol> <li> <p>Utilisation d'Expressions R\u00e9guli\u00e8res (Regex) :</p> <ul> <li>Principe : Rechercher des motifs textuels correspondant \u00e0 des variables ou des fonctions.</li> <li>Probl\u00e8me : Tr\u00e8s complexe \u00e0 maintenir, difficult\u00e9 \u00e0 distinguer le code des commentaires ou des cha\u00eenes de caract\u00e8res, et manque de fiabilit\u00e9 sur des structures complexes.</li> </ul> </li> <li> <p>Traitement de cha\u00eenes simples (String Manipulation) :</p> <ul> <li>Principe : D\u00e9couper le fichier par espaces ou retours \u00e0 la ligne.</li> <li>Probl\u00e8me : Ne comprend pas la syntaxe du langage (m\u00e9lange mots-cl\u00e9s du langage et noms de variables), tr\u00e8s bruit\u00e9.</li> </ul> </li> <li> <p>Analyse Syntaxique (Parsing AST) :</p> <ul> <li>Principe : Utiliser les outils natifs du langage pour transformer le code en un Arbre Syntaxique Abstrait (Abstract Syntax Tree).</li> <li>Avantage : Permet de comprendre structurellement le code (savoir ce qui est une fonction, une classe, une variable) et d'ignorer proprement la syntaxe pure (parenth\u00e8ses, mots-cl\u00e9s comme <code>def</code>, <code>class</code>).</li> </ul> </li> </ol>"},{"location":"parser/#solution-retenue","title":"Solution retenue","text":"<p>Nous avons opt\u00e9 pour la solution bas\u00e9e sur l'AST (Abstract Syntax Tree). </p> <ul> <li>Fiabilit\u00e9 : C'est la m\u00e9thode la plus robuste car elle utilise le m\u00eame m\u00e9canisme que l'interpr\u00e9teur Python lui-m\u00eame. Elle garantit que nous n'extrayons que des identifiants valides.</li> <li>Exemples connus : Cette technique est utilis\u00e9e par la majorit\u00e9 des outils d'analyse statique de code comme Pylint, Black ou les IDE pour l'autocompl\u00e9tion.</li> <li>Bonus : Cette technique est compatible avec plusieurs langages, dont certains sont v\u00e9rifi\u00e9s (C#, Go, Java, JavaScript, Python, etc.) et d'autres sont maintenus par des utilisateurs (Delphi, Ruby, PHP, etc.).</li> </ul>"},{"location":"parser/#difficultes-rencontrees","title":"Difficult\u00e9s rencontr\u00e9es","text":"<p>La mise en \u0153uvre a pr\u00e9sent\u00e9 quelques d\u00e9fis :</p> <ul> <li> <p>Distinction Identifiants vs Mots-cl\u00e9 : Il fallait s'assurer de ne r\u00e9cup\u00e9rer que les noms choisis par le d\u00e9veloppeur (noms de variables, fonctions) et non les mots r\u00e9serv\u00e9s du langage (<code>while</code>, <code>import</code>, etc.).</p> </li> <li> <p>Conventions de nommage h\u00e9t\u00e9rog\u00e8nes : Le code peut utiliser diff\u00e9rentes conventions (snake_case, camelCase). Un identifiant comme <code>calcul_moyenne</code> doit \u00eatre compris comme deux mots : \"calcul\" et \"moyenne\".</p> </li> <li> <p>Cha\u00eenes ou commentaires : En effet, les cha\u00eenes de caract\u00e8res et les commentaires peuvent contenir des mots, mais nous ne souhaitons pas syst\u00e9matiquement les r\u00e9cup\u00e9rer, car ils sont souvent utilis\u00e9s dans un contexte diff\u00e9rent du code et peuvent devenir des faux positifs. Si l'on souhaite les garder, on pourrait envisager d'appliquer un coefficient de priorit\u00e9 en fonction de la source du mot (important si c'est une variable, faible si c'est un commentaire ou une cha\u00eene de caract\u00e8res).</p> </li> <li> <p>Gestion des fichiers : Orchestrer la lecture du fichier source, le passage des donn\u00e9es entre les diff\u00e9rentes \u00e9tapes, et l'\u00e9criture du r\u00e9sultat final.</p> </li> </ul>"},{"location":"parser/#implementation-technique","title":"Impl\u00e9mentation technique","text":"<p>La solution a \u00e9t\u00e9 impl\u00e9ment\u00e9e sous la forme d'un pipeline compos\u00e9 de 4 scripts Python distincts, ex\u00e9cut\u00e9s s\u00e9quentiellement.</p>"},{"location":"parser/#1-orchestrator-orchestratorpy","title":"1. Orchestrator (<code>orchestrator.py</code>)","text":"<p>C'est le chef d'orchestre. Il ne r\u00e9alise pas de traitement mais coordonne l'appel des autres scripts. *   Fonctionnement : Il prend en entr\u00e9e le fichier source, appelle s\u00e9quentiellement l'extracteur, le d\u00e9coupeur, et l'analyseur de fr\u00e9quence. Il g\u00e8re le flux de donn\u00e9es entre chaque \u00e9tape pour s'assurer que la sortie de l'un devient l'entr\u00e9e du suivant.</p>"},{"location":"parser/#2-extractor-extractionpy","title":"2. Extractor (<code>extraction.py</code>)","text":"<p>Ce script est responsable de l'analyse syntaxique pure. *   Fonctionnement : Il lit le fichier source Python et utilise le module <code>ast</code> pour parcourir l'arbre syntaxique. Il visite les n\u0153uds du code (fonctions, classes, assignations) pour extraire tous les noms d'identifiants, en filtrant automatiquement les mots-cl\u00e9s du langage.</p>"},{"location":"parser/#3-splitter-splitterpy","title":"3. Splitter (<code>splitter.py</code>)","text":"<p>Ce script traite les cha\u00eenes de caract\u00e8res brutes extraites. *   Fonctionnement : Il prend une liste d'identifiants (ex: <code>userController</code>, <code>get_user_id</code>) et les d\u00e9compose en mots unitaires (<code>user</code>, <code>Controller</code>, <code>get</code>, <code>id</code>). Il g\u00e8re les diff\u00e9rentes casses :     *   snake_case (s\u00e9paration par underscores)     *   camelCase / PascalCase (s\u00e9paration par majuscules)     *   kebab-case (si pr\u00e9sent)</p>"},{"location":"parser/#4-frequency-analyzer-frequencypy","title":"4. Frequency Analyzer (<code>frequency.py</code>)","text":"<p>Ce dernier script compile les r\u00e9sultats. *   Fonctionnement : Il compte les occurrences de chaque mot normalis\u00e9. *   Sortie : Il g\u00e9n\u00e8re un fichier JSON final (nomm\u00e9 d'apr\u00e8s le fichier source) contenant les statistiques, par exemple : <code>{\"user\": 5, \"controller\": 2, \"id\": 8}</code>.</p>"},{"location":"parser/#resultat","title":"R\u00e9sultat","text":"<p>Afin de v\u00e9rifier le bon comportement du parser, nous avons r\u00e9alis\u00e9 diff\u00e9rent test sur les kathas Tennis cr\u00e9\u00e9 par Emily Bache.</p> <p>(https://github.com/emilybache/Tennis-Refactoring-Kata/tree/main/java/src/main/java)</p> <p>Comme dit pr\u00e9c\u00e9demment, le parser renvoie sous la forme d'un JSON le r\u00e9sultat, voici les r\u00e9sultats obtenue sur les kathas Tennis Game 1 \u00e0 7 (et l'interface). </p>"},{"location":"parser/#suite-et-perspectives","title":"Suite et Perspectives","text":"<p>Actuellement, le syst\u00e8me est fonctionnel mais pr\u00e9sente certaines limitations qui vont devoir \u00eatre corrig\u00e9es et am\u00e9lior\u00e9es pour la suite du projet :</p> <ul> <li>Support multi-langages : Pour l'instant, seul le langage Python est pris en charge (car nous utilisons le module <code>ast</code> sp\u00e9cifique \u00e0 Python). Pour supporter PHP, Java, C++, ou C#, nous devrons impl\u00e9menter des extracteurs sp\u00e9cifiques \u00e0 chaque langage ou utiliser une librairie de parsing polyglotte (comme Tree-sitter).</li> <li>Int\u00e9gration Front-end : Ces scripts fonctionnent actuellement en ligne de commande (CLI) ou via l'orchestrateur en backend. Ils ne sont pas encore connect\u00e9s \u00e0 l'interface utilisateur. Une API devra \u00eatre cr\u00e9\u00e9e pour permettre au front-end d'envoyer un fichier et de recevoir l'analyse JSON en retour.</li> <li>Lecture d'un programme entier : Actuellement, le programme n'analyse qu'un fichier donn\u00e9 \u00e0 la fois ; l'objectif est qu'il puisse recevoir un chemin ou un dossier source et parcourir l'enti\u00e8ret\u00e9 du dossier pour analyser tous les fichiers du langage concern\u00e9.</li> </ul>"},{"location":"recherches/","title":"Recherche : g\u00e9n\u00e9ration de glossaire","text":"<p>Comme expliqu\u00e9 dans l\u2019index du MkDocs, pour notre application, nous souhaitons utiliser une intelligence artificielle. N\u2019ayant aucune id\u00e9e de la mani\u00e8re de proc\u00e9der, nous avons effectu\u00e9 de nombreuses recherches (mod\u00e8les \u00e0 utiliser, cr\u00e9ation d\u2019un mod\u00e8le, mod\u00e8les d\u00e9j\u00e0 existants, appel \u00e0 des API, phase d\u2019entra\u00eenement, etc.). Tout l\u2019article recense nos recherches, leur \u00e9volution, leurs abandons \u00e9ventuels ou la justification de ces derni\u00e8res.</p>"},{"location":"recherches/#la-creation-dun-modele-dintelligence-artificielle","title":"La cr\u00e9ation d'un mod\u00e8le d'intelligence artificielle.","text":"<p>Avant de commencer le d\u00e9veloppement de notre application, il \u00e9tait essentiel de comprendre le fonctionnement d\u2019une intelligence artificielle et les diff\u00e9rentes \u00e9tapes n\u00e9cessaires \u00e0 sa cr\u00e9ation. Nous avons donc consacr\u00e9 une premi\u00e8re phase de notre travail \u00e0 la recherche th\u00e9orique, afin d\u2019acqu\u00e9rir une compr\u00e9hension solide des concepts fondamentaux li\u00e9s \u00e0 la conception et \u00e0 l\u2019entra\u00eenement d\u2019un mod\u00e8le d\u2019IA. Cette \u00e9tape nous a permis d\u2019identifier les \u00e9l\u00e9ments cl\u00e9s du processus d\u2019apprentissage automatique, notamment le r\u00f4le des hyperparam\u00e8tres et du taux d\u2019apprentissage dans la performance finale d\u2019un mod\u00e8le.</p>"},{"location":"recherches/#les-hyperparametres","title":"Les hyperparam\u00e8tres","text":"<p>Un hyperparam\u00e8tre est une valeur d\u00e9finie manuellement par le concepteur de l\u2019intelligence artificielle avant le d\u00e9but de l\u2019entra\u00eenement. Contrairement aux param\u00e8tres internes du mod\u00e8le (qui sont ajust\u00e9s automatiquement par l\u2019algorithme d\u2019apprentissage), les hyperparam\u00e8tres sont fix\u00e9s par l\u2019humain et influencent directement la mani\u00e8re dont le mod\u00e8le apprend.</p> <p>Ces hyperparam\u00e8tres peuvent contr\u00f4ler diff\u00e9rents aspects, comme :</p> <ul> <li>La taille du r\u00e9seau de neurones (nombre de couches ou de neurones),</li> <li>Le taux d\u2019apprentissage,</li> <li>Le nombre d\u2019it\u00e9rations ou d\u2019\u00e9poques d\u2019entra\u00eenement,</li> <li>Ou encore la taille des lots de donn\u00e9es trait\u00e9s \u00e0 chaque \u00e9tape.</li> </ul> <p>Un choix judicieux de ces valeurs est donc essentiel : de mauvais hyperparam\u00e8tres peuvent conduire \u00e0 un apprentissage inefficace, voire \u00e0 un \u00e9chec du mod\u00e8le.</p>"},{"location":"recherches/#le-taux-dapprentissage","title":"Le taux d\u2019apprentissage","text":"<p>Le taux d\u2019apprentissage (ou learning rate) est l\u2019un des hyperparam\u00e8tres les plus importants dans l\u2019entra\u00eenement d\u2019un mod\u00e8le d\u2019intelligence artificielle. Il d\u00e9termine la vitesse \u00e0 laquelle le mod\u00e8le ajuste ses param\u00e8tres internes au fil des it\u00e9rations.</p> <p>Un taux d\u2019apprentissage trop \u00e9lev\u00e9 risque d\u2019emp\u00eacher le mod\u00e8le de converger, c\u2019est-\u00e0-dire qu\u2019il n\u2019arrivera jamais \u00e0 une solution stable car il \u201csautera\u201d autour du minimum recherch\u00e9. \u00c0 l\u2019inverse, un taux trop faible rendra la convergence extr\u00eamement lente, n\u00e9cessitant un grand nombre de g\u00e9n\u00e9rations pour atteindre un r\u00e9sultat satisfaisant.</p> <p>Ainsi, il est n\u00e9cessaire de trouver un \u00e9quilibre entre rapidit\u00e9 d\u2019apprentissage et stabilit\u00e9 de convergence. Le taux d\u2019apprentissage peut \u00e9galement \u00eatre utilis\u00e9 pour d\u00e9terminer le nombre de g\u00e9n\u00e9rations ou d\u2019\u00e9poques d\u2019entra\u00eenement n\u00e9cessaires \u00e0 la stabilisation du mod\u00e8le.</p> <p>Figure 1 : Influence du taux d\u2019apprentissage sur la convergence d\u2019un mod\u00e8le d\u2019intelligence artificielle. </p> <p>Cette premi\u00e8re \u00e9tape nous a permis de mieux comprendre la structure interne d\u2019un mod\u00e8le d\u2019intelligence artificielle et les param\u00e8tres qui influencent son comportement. Ces connaissances ont constitu\u00e9 la base de notre travail, en nous aidant \u00e0 aborder plus sereinement les \u00e9tapes suivantes, notamment le choix et l\u2019entra\u00eenement d\u2019un mod\u00e8le adapt\u00e9 \u00e0 notre probl\u00e9matique de suggestion contextuelle de mots dans du code.</p>"},{"location":"recherches/#differents-modeles-testes","title":"Diff\u00e9rents mod\u00e8les test\u00e9s","text":"<p>Apr\u00e8s avoir acquis une compr\u00e9hension th\u00e9orique du fonctionnement d\u2019une intelligence artificielle et de ses param\u00e8tres essentiels, nous avons cherch\u00e9 \u00e0 identifier le mod\u00e8le le plus adapt\u00e9 \u00e0 notre probl\u00e9matique. Notre objectif \u00e9tait de d\u00e9velopper une IA capable de sugg\u00e9rer des synonymes ou des termes proches en fonction du contexte d\u2019un mot donn\u00e9 dans un code source. Pour cela, nous avons \u00e9tudi\u00e9 et exp\u00e9riment\u00e9 plusieurs mod\u00e8les de repr\u00e9sentation du langage afin d\u2019\u00e9valuer leur pertinence et leurs performances.</p>"},{"location":"recherches/#lutilisation-de-modeles-de-corpus-pre-entraines","title":"L\u2019utilisation de mod\u00e8les de corpus pr\u00e9-entra\u00een\u00e9s","text":"<p>Une premi\u00e8re approche a consist\u00e9 \u00e0 exploiter des mod\u00e8les de corpus pr\u00e9entra\u00een\u00e9s, c\u2019est-\u00e0-dire des mod\u00e8les ayant d\u00e9j\u00e0 appris les relations entre les mots \u00e0 partir de vastes ensembles de donn\u00e9es textuelles. Cette strat\u00e9gie permet de b\u00e9n\u00e9ficier de connaissances linguistiques d\u00e9j\u00e0 acquises, \u00e9vitant ainsi de devoir entra\u00eener un mod\u00e8le de z\u00e9ro \u2014 une t\u00e2che co\u00fbteuse en temps et en ressources.</p> <p>Nous avons notamment explor\u00e9 l\u2019utilisation de Word2Vec, un mod\u00e8le classique de traitement du langage naturel capable de repr\u00e9senter les mots sous forme de vecteurs num\u00e9riques (ou embeddings). Ces vecteurs traduisent les relations s\u00e9mantiques entre les mots : ainsi, les termes ayant un sens similaire se trouvent proches dans l\u2019espace vectoriel, tandis que ceux ayant des significations oppos\u00e9es ou diff\u00e9rentes s\u2019en \u00e9loignent.</p> <p>Cependant, nous avons constat\u00e9 que Word2Vec ne permettait pas toujours de distinguer correctement les relations de type synonyme ou antonyme, et qu\u2019il pouvait donc manquer de pr\u00e9cision dans certains cas d\u2019usage sp\u00e9cifiques \u00e0 notre domaine (le code informatique). D\u2019autres exp\u00e9rimentations sont pr\u00e9sent\u00e9es dans notre d\u00e9p\u00f4t GitHub (voir annexe technique).</p>"},{"location":"recherches/#choix-entre-pre-entrainement-et-modeles-existants","title":"Choix entre pr\u00e9-entra\u00eenement et mod\u00e8les existants","text":"<p>Une question importante s\u2019est ensuite pos\u00e9e : devions-nous entra\u00eener notre propre mod\u00e8le, adapt\u00e9 \u00e0 notre jeu de donn\u00e9es (le code source et ses identifiants), ou utiliser un mod\u00e8le d\u00e9j\u00e0 existant comme Word2Vec ou BERT ?</p> <p>L\u2019entra\u00eenement d\u2019un mod\u00e8le sur mesure aurait permis une meilleure adaptation au vocabulaire m\u00e9tier du code, mais demandait une quantit\u00e9 importante de donn\u00e9es et une puissance de calcul significative. \u00c0 l\u2019inverse, un mod\u00e8le pr\u00e9entra\u00een\u00e9 (comme BERT ou FastText) offrait une solution plus rapide \u00e0 mettre en \u0153uvre, tout en fournissant des r\u00e9sultats souvent pertinents dans des contextes g\u00e9n\u00e9raux.</p> <p>Nous avons donc d\u00e9cid\u00e9 de nous concentrer sur un seul algorithme \u00e0 la fois, afin d\u2019analyser plus en profondeur ses performances. Cette d\u00e9marche visait \u00e0 r\u00e9pondre \u00e0 une question centrale :</p> <p>Comment faciliter et am\u00e9liorer la s\u00e9lection de termes m\u00e9tiers pertinents dans un contexte donn\u00e9 ?</p> <p>Dans cette optique, l\u2019utilisation des embeddings (repr\u00e9sentations vectorielles des mots) s\u2019est r\u00e9v\u00e9l\u00e9e prometteuse pour la suggestion intelligente de termes. En mesurant la proximit\u00e9 s\u00e9mantique entre les vecteurs, il devient possible de proposer des synonymes coh\u00e9rents avec le contexte d\u2019apparition du mot dans le code.</p>"},{"location":"recherches/#experimentations-sur-les-relations-entre-mots","title":"Exp\u00e9rimentations sur les relations entre mots","text":"<p>Pour valider nos hypoth\u00e8ses, nous avons men\u00e9 plusieurs tests pratiques. L\u2019un d\u2019eux consistait \u00e0 prendre un mot de base, comme \u201cMoche\u201d, puis \u00e0 modifier son vecteur en lui ajoutant le vecteur repr\u00e9sentant une relation contraire (par exemple, un antonyme). L\u2019objectif \u00e9tait d\u2019observer le r\u00e9sultat vectoriel obtenu et de v\u00e9rifier si le mod\u00e8le parvenait \u00e0 proposer un mot oppos\u00e9 de mani\u00e8re coh\u00e9rente.</p> <p>Ces exp\u00e9rimentations ont permis d\u2019illustrer la capacit\u00e9 du mod\u00e8le \u00e0 manipuler les relations s\u00e9mantiques, mais aussi ses limites lorsqu\u2019il est appliqu\u00e9 \u00e0 des contextes techniques ou sp\u00e9cialis\u00e9s.</p> <p>Figure 2 : Exp\u00e9rience de transformation vectorielle du mot \u201cMoche\u201d vers son contraire. </p> <p>Cette phase d\u2019exp\u00e9rimentation nous a permis de comparer diff\u00e9rents mod\u00e8les linguistiques et d\u2019\u00e9valuer leur capacit\u00e9 \u00e0 comprendre le sens des mots dans un contexte pr\u00e9cis. Nous avons retenu que l\u2019utilisation d\u2019embeddings pr\u00e9entra\u00een\u00e9s constitue une base solide pour d\u00e9velopper une suggestion intelligente de mots, mais qu\u2019une adaptation au domaine du code informatique reste n\u00e9cessaire pour atteindre des r\u00e9sultats optimaux.</p>"},{"location":"recherches/#choix-pour-la-suggestion-evolution-et-limites","title":"Choix pour la suggestion, \u00e9volution et limites","text":""},{"location":"recherches/#premiere-reflexion-et-systeme-de-suggestion","title":"Premi\u00e8re r\u00e9flexion et syst\u00e8me de suggestion","text":"<p>Avec nos pr\u00e9c\u00e9dentes exp\u00e9rimentations, nous avons conclu que l'utilisation des embeddings pour analyser le contexte est pertinente, mais moins pr\u00e9cise pour sugg\u00e9rer directement des synonymes.</p> <p>Nous avons donc choisi de conserver les embeddings pour se rapprocher au maximum du contexte, mais de r\u00e9cup\u00e9rer une liste de synonymes depuis un service, qui sera ensuite compar\u00e9e au contexte.</p> <p>\u00c0 la base, comme nous avions la libert\u00e9 du langage, nous nous sommes pench\u00e9s sur le fran\u00e7ais. Le seul service permettant de r\u00e9cup\u00e9rer une liste de synonymes fran\u00e7ais sans passer par une traduction (Reverso) est ReSyf, qui propose \u00e9galement un code contenant les donn\u00e9es. C'est un service local, mais il poss\u00e8de une liste fig\u00e9e.</p> <p>Nous avons donc mis en place un syst\u00e8me o\u00f9 le mot cibl\u00e9 et les synonymes d\u00e9j\u00e0 saisis sont envoy\u00e9s au back-end ; celui-ci interroge d'abord le service ReSyf pour obtenir une liste de synonymes, puis combine ces r\u00e9sultats avec les synonymes d\u00e9j\u00e0 pr\u00e9sents pour calculer la proximit\u00e9 par rapport au contexte.</p>"},{"location":"recherches/#limite-de-lutilisation-dapi","title":"Limite de l'utilisation d'API","text":"<p>N\u00e9anmoins, cette solution atteint une certaine limite de pertinence lorsque l'on passe \u00e0 des contextes tr\u00e8s sp\u00e9cifiques : le fait que ReSyf soit fig\u00e9 devient bloquant. Par exemple, il n'y a pas de synonymes pour le contexte sp\u00e9cifique d'un plateau de jeu.</p> <p>De plus, pour chaque nouvelle langue que nous voulons ajouter, il nous faut un nouveau service de synonymes, ce qui limite l'efficacit\u00e9 et la vitesse d'impl\u00e9mentation.</p>"},{"location":"recherches/#nouvelle-solution","title":"Nouvelle solution","text":"<p>Le client a choisi \u00e0 la suite de cela, une nouvelle solution pour obtenir des synonymes et passer l'application en anglais. Nous nous sommes donc pench\u00e9s sur les mod\u00e8les de langages. Cependant, en tenant compte de la contrainte de conserver la souverainet\u00e9 des donn\u00e9es et de rester gratuit, l'utilisation d'API externes (ChatGPT, Gemini ou autres LLM r\u00e9pandus) n'est pas retenue. Il reste la possibilit\u00e9 d'utiliser un mod\u00e8le local, ce qui impose des contraintes de performance sur la machine qui h\u00e9berge l'application. </p> <p>C'est pourquoi nous nous sommes tourn\u00e9s vers des mod\u00e8les l\u00e9gers, inf\u00e9rieurs \u00e0 500 Mo : deux options se d\u00e9tachent \u2014 Gemma et Qwen3. Gemma est un mod\u00e8le li\u00e9 \u00e0 Google, mais il est compliqu\u00e9 \u00e0 faire tourner en local sans compte. Qwen3 est une solution viable, performante et qui tourne librement en local, avec une licence permettant son utilisation dans notre projet.</p>"},{"location":"technologie/","title":"Choix des technologies pour l'application","text":""},{"location":"technologie/#fastapi","title":"FastAPI","text":"<p>FastAPI est un framework web moderne pour Python, con\u00e7u pour cr\u00e9er des API. Il est particuli\u00e8rement appr\u00e9ci\u00e9 pour sa rapidit\u00e9 d\u2019ex\u00e9cution, proche de celle de Node.js ou Go, et sa facilit\u00e9 d\u2019utilisation qui permet d\u2019\u00e9crire du code clair. Gr\u00e2ce \u00e0 la d\u00e9claration de types Python, FastAPI g\u00e9n\u00e8re automatiquement une documentation interactive.</p>"},{"location":"technologie/#le-type-dapplication","title":"Le type d'application","text":"<p>Initialement, nous avions envisag\u00e9 de d\u00e9velopper notre application sous forme de site web. Cette approche semblait id\u00e9ale, car le client souhaitait une application accessible rapidement par tous, depuis n\u2019importe quel appareil. Cependant, apr\u00e8s r\u00e9flexion, elle pr\u00e9sentait un inconv\u00e9nient majeur : la souverainet\u00e9 des donn\u00e9es.</p> <p>La souverainet\u00e9 des donn\u00e9es, c'est le fait de pouvoir garder le contr\u00f4le sur ses informations, savoir o\u00f9 elles sont stock\u00e9es et qui peut y acc\u00e9der, afin de prot\u00e9ger leur confidentialit\u00e9 et leur s\u00e9curit\u00e9.</p> <p>En effet, notre application devait \u00eatre utilis\u00e9e \u00e0 la fois par des \u00e9tudiants de l\u2019IUT et par des utilisateurs externes. Un site web impliquait de stocker les donn\u00e9es sur des serveurs distants, ce qui pouvait poser des probl\u00e8mes de s\u00e9curit\u00e9 et de confidentialit\u00e9.</p> <p>Nous avons donc \u00e9tudi\u00e9 plusieurs alternatives, et finalement, nous avons opt\u00e9 pour une application de bureau. Cette solution pr\u00e9sente plusieurs avantages. Tout d'abord, les donn\u00e9es peuvent \u00eatre stock\u00e9es localement, garantissant un meilleur contr\u00f4le et une meilleure confidentialit\u00e9. De plus, l\u2019application reste accessible aux \u00e9tudiants de l\u2019IUT comme aux utilisateurs externes, sans d\u00e9pendre d\u2019un h\u00e9bergement web.</p> <p>Ainsi, le choix de l\u2019application de bureau r\u00e9pond \u00e0 nos exigences en termes de s\u00e9curit\u00e9, de souverainet\u00e9 des donn\u00e9es et d\u2019accessibilit\u00e9.</p>"},{"location":"technologie/#choix-de-la-technologie-pour-lapplication-de-bureau","title":"Choix de la technologie pour l\u2019application de bureau","text":"<p>Apr\u00e8s le choix d'opter pour une application de bureau, nous avons r\u00e9fl\u00e9chi \u00e0 quelle technologie nous allions utiliser pour la d\u00e9velopper.</p> <p>C'est pourquoi nous avons compar\u00e9 plusieurs solutions : Flet, Electron et Tauri. Premi\u00e8rement, Flet est tr\u00e8s r\u00e9cent et pr\u00e9sente donc une documentation limit\u00e9e, ce qui rend son utilisation risqu\u00e9e pour le projet. En effet, la premi\u00e8re version de Flet consid\u00e9r\u00e9e comme stable est sortie en 2025. Ensuite, Electron est tr\u00e8s populaire car il est utilis\u00e9 par des applications r\u00e9put\u00e9es comme Discord, ce qui t\u00e9moigne de sa fiabilit\u00e9. De plus, il est facile \u00e0 prendre en main. Cependant, il consomme en moyenne 200 \u00e0 300\u202fMo de RAM au d\u00e9marrage, contre seulement 30 \u00e0 50\u202fMo pour Tauri, ce qui peut impacter les performances. \u00c0 l\u2019inverse, Tauri combine l\u00e9g\u00e8ret\u00e9, performance et une documentation suffisamment compl\u00e8te, ce qui nous garantit une bonne exp\u00e9rience utilisateur. De plus, au lancement de l'application, Tauri s\u2019est av\u00e9r\u00e9 3 \u00e0 4 fois plus rapide au d\u00e9marrage que Flet et Electron sur des machines standards. Tauri n\u2019est pas encore aussi largement adopt\u00e9 que Electron, mais il existe des applications connues qui ont \u00e9t\u00e9 d\u00e9velopp\u00e9es avec Tauri. Par exemple Zed, un \u00e9diteur de code, et Nota, une application de prise de notes bas\u00e9e sur Markdown.</p>"},{"location":"technologie/#embarquement-de-lintelligence-artificielle-locale","title":"Embarquement de l'intelligence artificielle locale","text":"<p>Puisqu'on utilise une IA qui tourne en local pour l'application, il faut embarquer l'IA dans celle-ci. On a vu deux options pour cela : soit embarquer directement le mod\u00e8le via un fichier .gguf qui contient le mod\u00e8le, soit passer par un gestionnaire comme Ollama. L'approche .gguf peut cependant r\u00e9duire les performances du mod\u00e8le. Nous avons donc privil\u00e9gi\u00e9 une autre approche : Ollama, qui permet de t\u00e9l\u00e9charger de nombreux mod\u00e8les et de les faire tourner en local tr\u00e8s facilement. Le seul inconv\u00e9nient est d'installer l'application Ollama, c'est pourquoi on d\u00e9tecte sa pr\u00e9sence et on propose de l'installer si elle n'est pas pr\u00e9sente sur l'ordinateur. Cela pr\u00e9sente \u00e9galement un autre avantage : on peut changer de mod\u00e8le tr\u00e8s facilement et proposer \u00e0 l'avenir d'utiliser un mod\u00e8le plus gros pour de meilleures performances si l'utilisateur a un ordinateur plus puissant.</p>"},{"location":"technologie/#generation-des-builders","title":"G\u00e9n\u00e9ration des builders","text":"<p>Pour g\u00e9n\u00e9rer automatiquement les builders (installateurs et paquets) pour Linux, macOS et Windows, nous avons utilis\u00e9 un workflow GitHub Actions. </p> <p>Pour celui-ci nous avons utilis\u00e9 un fichier YAML :</p> <ul> <li> <p>Cr\u00e9\u00e9 et affin\u00e9 avec l'aide d'un outil d'IA g\u00e9n\u00e9rative</p> </li> <li> <p>Orchestrant la compilation et le packaging sur chaque plateforme.</p> </li> </ul> <p>Le workflow se d\u00e9clenche lors d'un push de version contenant le suffixe \u00ab -v \u00bb, ce qui lance la cr\u00e9ation des builders et la publication des artefacts.</p>"},{"location":"technologie/#conclusion","title":"Conclusion","text":"<p>Au terme de notre analyse, Tauri s\u2019est impos\u00e9 comme la solution la plus adapt\u00e9e pour notre application de bureau. Son \u00e9quilibre entre performance, l\u00e9g\u00e8ret\u00e9 et fiabilit\u00e9 en fait un choix pertinent pour offrir une exp\u00e9rience utilisateur optimale tout en garantissant la s\u00e9curit\u00e9 et la gestion des donn\u00e9es. Parall\u00e8lement, FastAPI nous permet de g\u00e9rer efficacement le back-end de l\u2019application, gr\u00e2ce \u00e0 sa rapidit\u00e9, sa clart\u00e9 et la g\u00e9n\u00e9ration automatique de documentation, assurant ainsi une communication fluide entre l\u2019interface et les donn\u00e9es.</p>"}]}